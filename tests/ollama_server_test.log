2025-05-16 10:00:00.001 | INFO     | config.config_manager:log_info:12 - [CONFIG_MANAGER] [load_config] [load] Feature: config_manager | File: config.yaml | Loaded config from config.yaml
2025-05-16 10:00:00.123 | INFO     | llm.ollama_client:log_info:17 - [LLM_CLIENT] [__init__] [init] Feature: LLM | File: analysis/llm_client.py | OllamaCodeLlama initialized with model=codellama:7b and host=http://localhost:11434
2025-05-16 10:00:00.456 | INFO     | server.server_manager:log_info:16 - [SERVER_MANAGER] [start_server] [startup] Feature: Server | File: http_api.py | Starting server with Gunicorn in the background on port 5000...
2025-05-16 10:00:01.000 | INFO     | http_api:log_info:52 - [HTTP_API] [Startup] [startup] Feature: API | File: http_api.py | Server ready to accept requests.
2025-05-16 10:00:01.500 | INFO     | cli:log_info:65 - [CLI] [generate_github_report] [start] Feature: ReportGen | File: reports/github_audit_test.md.partial | Starting GitHub report generation.
2025-05-16 10:00:02.000 | INFO     | github_audit:log_info:31 - [GITHUB_AUDIT] [async_analyze_code_files] [start] Feature: FileAnalysis | File: src/utils.py | Starting analysis for src/utils.py
2025-05-16 10:00:02.500 | INFO     | llm.ollama_client:log_info:17 - [LLM_CLIENT] [async_generate] [start] Feature: LLM | File: analysis/llm_client.py | PromptHash: 123abc | [async] Generating response for prompt: def foo(bar): ...
2025-05-16 10:01:12.123 | WARNING  | llm.ollama_client:log_warning:19 - [LLM_CLIENT] [async_generate] [json_parse_single] Feature: LLM | File: analysis/llm_client.py | PromptHash: 123abc | Failed to parse as single JSON object: Extra data: line 2 column 1 (char 97)
2025-05-16 10:01:12.124 | WARNING  | llm.ollama_client:log_warning:19 - [LLM_CLIENT] [async_generate] [llm_response_truncated] Feature: LLM | File: analysis/llm_client.py | PromptHash: 123abc | First 500 chars of LLM response: {"model":"codellama:7b","created_at":"2025-05-16T10:01:12Z","response":"\n","done":false}
{"model":"codellama:7b","created_at":"2025-05-16T10:01:13Z","response":"Test","done":false}
2025-05-16 10:01:13.555 | ERROR    | llm.ollama_client:log_error:21 - [LLM_CLIENT] [async_generate] [call] Feature: LLM | File: analysis/llm_client.py | PromptHash: 123abc | Ollama LLM async error: Connection reset by peer
2025-05-16 10:01:14.789 | ERROR    | github_audit:log_error:37 - [GITHUB_AUDIT] [async_analyze_code_files] [write_file_section] Feature: FileAnalysis | File: src/utils.py | PromptHash: 456def | Failed to write analysis for src/utils.py: [Errno 13] Permission denied: 'reports/github_audit_test.md.partial'
2025-05-16 10:01:15.001 | WARNING  | cli:log_warning:68 - [CLI] [generate_github_report] [write_report] Feature: ReportGen | File: reports/github_audit_test.md.partial | PromptHash: 789ghi | Report file is empty after generation.
2025-05-16 10:01:15.123 | ERROR    | config.config_manager:log_error:15 - [CONFIG_MANAGER] [load_config] [load] Feature: config_manager | File: config.yaml | Error loading config: FileNotFoundError: [Errno 2] No such file or directory: 'config.yaml'
2025-05-16 10:01:16.456 | WARNING  | server.server_manager:log_warning:18 - [SERVER_MANAGER] [start_server] [startup] Feature: Server | File: http_api.py | Port 5000 already in use. Server not started.
2025-05-16 10:01:17.789 | ERROR    | reports.report_manager:log_error:14 - [REPORT_MANAGER] [generate_markdown_report] [write] Feature: ReportGen | File: reports/github_audit_test.md | PromptHash: 789ghi | UnicodeEncodeError: 'ascii' codec can't encode character '\u2014' in position 123: ordinal not in range(128)
2025-05-16 10:01:18.321 | WARNING  | llm.ollama_client:log_warning:19 - [LLM_CLIENT] [async_generate] [timeout] Feature: LLM | File: analysis/llm_client.py | PromptHash: 123abc | LLM call timed out after 120s.
2025-05-16 10:01:19.654 | ERROR    | utils.helpers:log_error:16 - [HELPERS] [analyze_files_parallel] [cache] Feature: Cache | File: .cache/test.json | PromptHash: 999zzz | Failed to read cache: JSONDecodeError: Expecting value: line 1 column 1 (char 0) 